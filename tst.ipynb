{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "mnist_ds= torchvision.datasets.MNIST(DATA, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make all desired image transformations here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "custom DS for loading tensors and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDS(Dataset):\n",
    "    def __init__(self, mnist_ds, transform):\n",
    "        super().__init__()\n",
    "        self.mnist_ds= mnist_ds\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_ds)\n",
    "    def __getitem__(self, i):\n",
    "        return self.transform(self.mnist_ds[i][0]), self.mnist_ds[i][1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = MNISTDS(mnist_ds, transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DCGAN architecture is a bunch of stacked \"transposed convolutions\". If like me, you're wondering what the hell a tranposed convolution is, see [here](https://github.com/vdumoulin/conv_arithmetic) for some very helpful visualizations. The punchline is: it's an ordinary convolution, but where the *stride* is used to \"inflate\" the input image before feeding it to the conv filter, so that the outputs can end up being larger in spatial extent than the inputs.\n",
    "\n",
    "I'm basically copying the [example pytorch implementation of DCGAN](https://github.com/pytorch/examples/blob/master/dcgan/main.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_size):\n",
    "        \"\"\"latent_size = size of the latent space\"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.latent_size = latent_size\n",
    "        \n",
    "        #spatial extent at each layer\n",
    "        size = [4, 8, 16, 32]\n",
    "        #kernel size\n",
    "        self.kernel_size =4\n",
    "        #(proportional to the) number of generator filters\n",
    "        self.ngf = 64\n",
    "        \n",
    "        #takes a latent vector and outputs MNIST-sized image\n",
    "        #input: (_, s, 1, 1) latent vector\n",
    "        self.upsample = nn.Sequential(\n",
    "            \n",
    "                                nn.ConvTranspose2d(self.latent_size, 4 * self.ngf, self.kernel_size,\n",
    "                                                      stride=1,padding=0), \n",
    "                                nn.BatchNorm2d( 4 * self.ngf), \n",
    "                                nn.ReLU(),\n",
    "                                #spatial extent here is set by the kernel: (4,4)\n",
    "                                \n",
    "                                #by setting stride=2, we effectively double the output size (up to fiddling\n",
    "                                #with the boundary conditions..)\n",
    "                                # Weirdly, increasing the 'padding' arg actually decreases the amount of padding \n",
    "                                #that's applied to the input. the only reason padding is being used here is to\n",
    "                                #keep the output shapes at nice multiples of two\n",
    "                                nn.ConvTranspose2d(4 * self.ngf, 2 * self.ngf, self.kernel_size,\n",
    "                                                      stride=2,padding=1), \n",
    "                                nn.BatchNorm2d( 2 * self.ngf), \n",
    "                                nn.ReLU(),\n",
    "                                \n",
    "                                #( 8,8)\n",
    "                                nn.ConvTranspose2d(2 * self.ngf, 1 * self.ngf, self.kernel_size,\n",
    "                                                      stride=2,padding=1), \n",
    "                                nn.BatchNorm2d( 1 * self.ngf), \n",
    "                                nn.ReLU(),\n",
    "                                #(16,16)\n",
    "                                #here I'm increasing the padding to bring the output size to (28,28)\n",
    "                                #for MNIST\n",
    "                                nn.ConvTranspose2d(self.ngf, 1, self.kernel_size,\n",
    "                                                      stride=2,padding=3), \n",
    "                                nn.Tanh(),\n",
    "                                #(32,32)\n",
    "                                \n",
    "                                                                                            \n",
    "                                )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        \"\"\"Input: (_, latent_size) noise tensor\n",
    "            Output: (_, 1, 32, 32) generated image tensor\"\"\"\n",
    "        z = z.view(-1, self.latent_size, 1, 1)\n",
    "        return self.upsample(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        #scaling for the number of filters\n",
    "        self.nf = 64\n",
    "        # kernel size \n",
    "        self.kernel_size = 4\n",
    "        #input (1,28,28)\n",
    "        \n",
    "        #this is more or less the generator stack run in reverse\n",
    "        # a stride of 2 and padding of 1 causes the spatial extent to halve at each step\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(1, self.nf, self.kernel_size, stride=2,padding=3),\n",
    "            nn.LeakyReLU(.2),\n",
    "            nn.Conv2d(self.nf, 2 * self.nf, self.kernel_size, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(2 * self.nf),\n",
    "            nn.LeakyReLU(.2),\n",
    "            nn.Conv2d(2*self.nf, 4 * self.nf, self.kernel_size, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(4 * self.nf),\n",
    "            nn.LeakyReLU(.2),\n",
    "            nn.Conv2d(4*self.nf, 1 , self.kernel_size, stride=1, padding=0),\n",
    "            nn.Sigmoid()\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Input: (_, 1, 28, 28) image\n",
    "            Output: (_, 1) classification tensor\"\"\"\n",
    "        x = x.view(-1, 1, 28,28)\n",
    "        return self.main(x).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz = 10\n",
    "z = torch.ones(1,nz,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.ones(3, 1, 28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator(nz)\n",
    "D = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = D(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
